# 
trained on a massive 40GB dataset called WebText 
GPT2 uses transformer decoder blocks.  
BERT uses transformer encoder blocks.  


# References
- [gpt2 notes](https://jalammar.github.io/illustrated-gpt2/)