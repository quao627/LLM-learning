{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, set_seed\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# specifying device_map=\"auto\" will be good enough as ðŸ¤— Accelerate will attempt to fill all the space in your GPU(s), then loading them to the CPU, \n",
    "# and finally if there is not enough RAM it will be loaded to the disk\n",
    "\n",
    "pipe = pipeline('text-generation', model='gpt2', device_map=\"auto\")\n",
    "output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95, max_length=30, truncation=True, num_return_sequences=5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[31373,   995]]), 'attention_mask': tensor([[1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "sequence = \"hello world\"\n",
    "encoded_input = tokenizer(sequence, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "output = model(**encoded_input)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "decoded_sequence = tokenizer.decode(encoded_input[\"input_ids\"][0].tolist())\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8530/8530 [00:00<00:00, 27576.64 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1066/1066 [00:00<00:00, 21821.66 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1066/1066 [00:00<00:00, 21286.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4560c294051a3d1f5a575d4d33347931c18dbfb5\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_output\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\") \n",
    "dataset = dataset.map(tokenize_dataset, batched=True) # use this map function\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# combine all \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")  # doctest: +SKIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnexa4ai\u001b[0m (\u001b[33mnexaai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zack/self-learn/LLM-learning/huggingface/wandb/run-20240318_195739-eulw9ts6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nexaai/huggingface/runs/eulw9ts6' target=\"_blank\">easy-sun-2</a></strong> to <a href='https://wandb.ai/nexaai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nexaai/huggingface' target=\"_blank\">https://wandb.ai/nexaai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nexaai/huggingface/runs/eulw9ts6' target=\"_blank\">https://wandb.ai/nexaai/huggingface/runs/eulw9ts6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2134' max='2134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2134/2134 00:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2134, training_loss=0.3402043586595101, metrics={'train_runtime': 60.7028, 'train_samples_per_second': 281.041, 'train_steps_per_second': 35.155, 'total_flos': 195974132394480.0, 'train_loss': 0.3402043586595101, 'epoch': 2.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can customize the training loop behavior by subclassing the methods inside Trainer. \n",
    "# This allows you to customize features such as the loss function, optimizer, and scheduler. \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch models and tokenizers to use offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./your/path/bigscience_t0\")\n",
    "model = AutoModel.from_pretrained(\"./your/path/bigscience_t0\")\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"bigscience/T0_3B\", filename=\"config.json\", cache_dir=\"./bigscience_t0\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"./your/path/bigscience_t0/config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\")\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained instances with an AutoClass\n",
    "1. Text, use a Tokenizer to convert text into a sequence of tokens  \n",
    "2. Speech and audio, use a Feature extractor to extract sequential features from audio waveforms and convert them into tensors.  \n",
    "3. Image inputs use a ImageProcessor to convert images into tensors.  \n",
    "4. Multimodal inputs, use a Processor to combine a tokenizer and a feature extractor or image processor.  \n",
    "\n",
    "- Load a pretrained tokenizer.  \n",
    "- Load a pretrained image processor  \n",
    "- Load a pretrained feature extractor.  \n",
    "- Load a pretrained processor.  \n",
    "- Load a pretrained model.  \n",
    "- Load a model as a backbone.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "\"\"\"\n",
    "input_ids are the indices corresponding to each token in the sentence.\n",
    "attention_mask indicates whether a token should be attended to or not.\n",
    "token_type_ids identifies which sequence a token belongs to when there is more than one sequence.\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "sequence = \"In a hole in the ground there lived a hobbit.\"\n",
    "encoded_input = tokenizer(sequence)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] in a hole in the ground there lived a hobbit. [SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tokenizer added two special tokens - CLS and SEP (classifier and separator) - to the sentence. \n",
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding token\n",
    "Padding is a strategy for ensuring tensors are rectangular by adding a special padding token to shorter sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2021,  2054,  2055,  2117,  6350,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2123,  1005,  1056,  2228,  2002,  4282,  2055,  2117,  6350,\n",
      "          1010, 28315,  1012,   102],\n",
      "        [  101,  2054,  2055,  5408, 14625,  1029,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import load_dataset, AutoTokenizer\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
